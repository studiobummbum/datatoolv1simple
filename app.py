import streamlit as st
import pandas as pd
import io

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="AdMob Cohort Analyzer Pro", layout="wide")

st.title("üí∞ AdMob Cohort LTV Analyzer (V3.4 - Bulletproof)")
st.markdown("""
<style>
    .stAlert { padding: 10px; border-radius: 5px; }
    .success { background-color: #d4edda; color: #155724; }
</style>
""", unsafe_allow_html=True)

st.info("üí° Upload file `admob-report.csv`. H·ªá th·ªëng t·ª± ƒë·ªông nh·∫≠n di·ªán header v√† encoding.")

# --- H√ÄM X·ª¨ L√ù DATA ---
def load_data(uploaded_file):
    # Danh s√°ch encoding hay g·∫∑p c·ªßa AdMob/Excel
    encodings = ['utf-8', 'utf-16', 'latin1', 'iso-8859-1', 'cp1252']
    
    df = None
    used_encoding = None
    header_row = 0
    
    # 1. Th·ª≠ ƒë·ªçc v·ªõi c√°c encoding v√† v·ªã tr√≠ header kh√°c nhau
    # AdMob th∆∞·ªùng c√≥ 2 d√≤ng ƒë·∫ßu l√† Title, d√≤ng 3 m·ªõi l√† Header (skiprows=2)
    # Nh∆∞ng file s·∫øp g·ª≠i c√≥ th·ªÉ Header n·∫±m ngay d√≤ng 0
    
    possible_skiprows = [0, 2] # ∆Øu ti√™n d√≤ng 0 tr∆∞·ªõc theo file m·∫´u s·∫øp g·ª≠i
    
    for skip in possible_skiprows:
        for enc in encodings:
            try:
                uploaded_file.seek(0)
                temp_df = pd.read_csv(uploaded_file, skiprows=skip, encoding=enc, on_bad_lines='skip')
                
                # Check nhanh xem c√≥ c·ªôt n√†o tr√¥ng gi·ªëng Date ho·∫∑c Country kh√¥ng
                col_str = " ".join([str(c).lower() for c in temp_df.columns])
                if 'date' in col_str and ('country' in col_str or 'install' in col_str):
                    df = temp_df
                    used_encoding = enc
                    header_row = skip
                    break
            except:
                continue
        if df is not None:
            break
            
    return df, used_encoding, header_row

# --- UI CH√çNH ---
uploaded_file = st.file_uploader("üìÇ K√©o th·∫£ file CSV v√†o ƒë√¢y s·∫øp ∆°i", type=['csv'])

if uploaded_file is not None:
    with st.spinner('ƒêang soi data c·ªßa s·∫øp...'):
        df, encoding, header_row = load_data(uploaded_file)

    if df is None:
        st.error("‚ùå Em ch·ªãu thua! Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file. S·∫øp check l·∫°i xem c√≥ ph·∫£i CSV chu·∫©n kh√¥ng?")
        st.stop()

    # --- X·ª¨ L√ù T√äN C·ªòT (MAPPING) ---
    # Chu·∫©n h√≥a t√™n c·ªôt hi·ªán t·∫°i v·ªÅ ch·ªØ th∆∞·ªùng, b·ªè kho·∫£ng tr·∫Øng th·ª´a
    df.columns = df.columns.astype(str).str.strip()
    
    # Dictionary t·ª´ kh√≥a ƒë·ªÉ map (∆Øu ti√™n t·ª´ kh√≥a d√†i tr∆∞·ªõc)
    # File s·∫øp: "Install date", "Install country", "Days since install", "LTV (USD)"
    mapping_rules = {
        'Date': ['install date', 'date', 'ng√†y'],
        'Country': ['install country', 'country', 'qu·ªëc gia', 'region'],
        'Day': ['days since install', 'day', 'ng√†y k·ªÉ t·ª´'],
        'LTV': ['ltv', 'revenue', 'doanh thu', 'earnings'],
        'Installs': ['installs', 'l∆∞·ª£t c√†i ƒë·∫∑t', 'c√†i ƒë·∫∑t']
    }

    final_rename_map = {}
    found_cols = []

    # Logic t√¨m c·ªôt th√¥ng minh
    for target_name, keywords in mapping_rules.items():
        match_col = None
        for col in df.columns:
            # So s√°nh case-insensitive
            if any(kw in col.lower() for kw in keywords):
                # Logic lo·∫°i tr·ª´ ƒë·∫∑c bi·ªát cho c·ªôt Installs (tr√°nh nh·∫ßm v·ªõi Install Date)
                if target_name == 'Installs' and ('date' in col.lower() or 'day' in col.lower() or 'country' in col.lower()):
                    continue
                match_col = col
                break
        
        if match_col:
            final_rename_map[match_col] = target_name
            found_cols.append(target_name)

    # --- HI·ªÇN TH·ªä TR·∫†NG TH√ÅI MAPPING (DEBUG) ---
    with st.expander("üïµÔ∏è‚Äç‚ôÇÔ∏è Debug: Em ƒë√£ map c√°c c·ªôt nh∆∞ th·∫ø n√†y (S·∫øp check nh√©)"):
        st.write(f"**Encoding:** `{encoding}` | **Header Row:** `{header_row}`")
        st.json(final_rename_map)
        st.write("Data g·ªëc 5 d√≤ng ƒë·∫ßu:")
        st.dataframe(df.head())

    # Ki·ªÉm tra c·ªôt b·∫Øt bu·ªôc
    required_cols = ['Date', 'Day', 'LTV']
    missing = [col for col in required_cols if col not in found_cols]
    
    if missing:
        st.error(f"‚ùå Toang r·ªìi s·∫øp ∆°i! Em kh√¥ng t√¨m th·∫•y c·ªôt: {', '.join(missing)}")
        st.stop()

    # --- √ÅP D·ª§NG RENAME ---
    df = df.rename(columns=final_rename_map)

    # --- CLEAN DATA TYPES ---
    try:
        # 1. Date
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        df = df.dropna(subset=['Date'])

        # 2. LTV & Installs (X·ª≠ l√Ω d·∫•u ph·∫©y, d·∫•u $)
        cols_to_numeric = ['LTV']
        if 'Installs' in df.columns:
            cols_to_numeric.append('Installs')
        
        for col in cols_to_numeric:
            if df[col].dtype == object:
                df[col] = df[col].astype(str).str.replace(r'[$,‚Ç´a-zA-Z()]', '', regex=True)
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        if 'Installs' not in df.columns:
            df['Installs'] = 1 # Fallback n·∫øu kh√¥ng c√≥ c·ªôt install

    except Exception as e:
        st.error(f"‚ùå L·ªói khi clean data: {e}")
        st.stop()

    # --- PIVOT TABLE (COHORT) ---
    # Ch·ªâ l·∫•y c√°c ng√†y quan tr·ªçng
    target_days = [0, 1, 3, 7, 14, 28, 30, 60]
    df_filtered = df[df['Day'].isin(target_days)].copy()

    # Group by ƒë·ªÉ t√≠nh t·ªïng LTV theo Date v√† Country
    # L∆∞u √Ω: File s·∫øp l√† d·∫°ng flat (m·ªói d√≤ng 1 ng√†y), c·∫ßn pivot
    
    # N·∫øu kh√¥ng c√≥ c·ªôt Country (tr∆∞·ªùng h·ª£p x·∫•u nh·∫•t), t·∫°o c·ªôt All
    if 'Country' not in df.columns:
        df_filtered['Country'] = 'Global'

    # Pivot: Index=(Date, Country, Installs), Col=Day, Val=LTV
    # C·∫ßn aggregate Installs theo Date+Country tr∆∞·ªõc (v√¨ Installs l·∫∑p l·∫°i ·ªü m·ªói d√≤ng Day 0,1,2...)
    # Logic chu·∫©n: L·∫•y Installs t·∫°i Day 0 l√†m g·ªëc cho Cohort ƒë√≥
    
    df_installs = df[df['Day'] == 0][['Date', 'Country', 'Installs']].drop_duplicates()
    
    # Pivot LTV
    df_ltv = df_filtered.pivot_table(
        index=['Date', 'Country'],
        columns='Day',
        values='LTV',
        aggfunc='sum'
    ).reset_index()
    
    # Merge Installs v√†o b·∫£ng LTV
    final_df = pd.merge(df_installs, df_ltv, on=['Date', 'Country'], how='left')
    
    # ƒê·ªïi t√™n c·ªôt LTV D...
    new_cols = {d: f'LTV D{d}' for d in target_days if d in final_df.columns}
    final_df = final_df.rename(columns=new_cols)
    
    # Fill NaN = 0
    final_df = final_df.fillna(0)
    
    # S·∫Øp x·∫øp
    final_df = final_df.sort_values(by='Date', ascending=False)

    # --- HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
    st.success("‚úÖ X·ª≠ l√Ω xong! M·ªùi s·∫øp x∆°i.")
    
    # Format hi·ªÉn th·ªã
    format_config = {'Installs': '{:,.0f}'}
    ltv_cols = [c for c in final_df.columns if 'LTV' in c]
    for c in ltv_cols:
        format_config[c] = '${:.4f}'

    st.dataframe(
        final_df.style.format(format_config)
        .background_gradient(cmap='Greens', subset=ltv_cols),
        use_container_width=True,
        height=600
    )
    
    # T√≠nh t·ªïng ARPU Global
    st.subheader("üìà T·ªïng h·ª£p ARPU (Weighted Average)")
    total_installs = final_df['Installs'].sum()
    if total_installs > 0:
        avg_data = {}
        for col in ltv_cols:
            # T√≠nh t·ªïng doanh thu c·ªßa c·ªôt ƒë√≥ / t·ªïng install
            # L∆∞u √Ω: ƒê√¢y l√† t√≠nh trung b√¨nh c·ªông gia quy·ªÅn
            revenue_col = (final_df[col] * final_df['Installs']).sum()
            arpu = revenue_col / total_installs
            avg_data[col] = arpu
            
        st.metric("Total Installs", f"{total_installs:,.0f}")
        st.dataframe(pd.DataFrame([avg_data]).style.format('${:.4f}'))